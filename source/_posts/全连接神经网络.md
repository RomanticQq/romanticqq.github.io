---
title: 全连接神经网络
tags:
  - 计算机视觉
  - 图像处理
  - 深度学习
  - 全连接神经网络
categories: 计算机视觉
abbrlink: 13828
date: 2021-09-18 16:02:26
---

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918161320.png)

### 0.像素表示

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918161649.png)

如（r1,g1,b1,r2,g2,b2...rn,gn,bn)这样

### 1.多层感知机

#### 全连接神经网络

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918161939.png)

max()叫激活函数。

激活函数不能去掉，去掉就退化成线性分类器。

#### 全连接神经网络的权值

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918162041.png)

w1可以看做模板，它可以有比类别数更多的分类，就是一个类别可以有多个模板。

比如10中类别的动物，w1可以有50中模板甚至更多，即同一类动物可以有多个模板，把与x得到的结果与w2融合，找到最大的，得出结果。

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918162213.png)

线性分类器的w行数和类别数相同，而全连接神经网络中只需最外层w的行数和类别数相同。

#### 全连接神经网络与线性不可分

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918162318.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918162404.png)

线性分类器可用于分离线性可分的，但是线性分类器不能把线性不可分的分开，此时就需要有全连接神经网络这样的模型。

#### 全连接神经网络绘制与命名

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918162526.png)

每个x相互独立为单独的输入。

### 2.激活函数

#### 激活函数

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918164220.png)

#### 常用的激活函数

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918164319.png)

sigmoid函数的范围为(0,1);

tanh函数的范围是(-1,1);而且是中心对称的；

#### 网络结构设计

1. **用不用隐层，用一个还是用几个隐层？（深度设计）** 

2. **每隐层设置多少个神经元比较合适？（宽度设计)**

   没有确定的答案。

   ![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918165009.png)

   依据分类任务的难易程度来调整神经网络模型的复杂程度。分 类任务越难，我们设计的神经网络结构就应该越深、越宽。但是， 需要注意的是对训练集分类精度最高的全连接神经网络模型，在 真实场景下识别性能未必是最好的（过拟合） 。

#### 全连接神经网络小结

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918164808.png)

### 3.softmax与交叉熵

#### softmax函数

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918165313.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918165521.png)

softmax函数是把分类的结果得分转换成各种结果的概率。

#### 交叉熵

在此处交叉熵的意义在于衡量两个分布的不相似度。

计算交叉熵之前要用softmax方法做归一化处理。

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918165825.png)

相对熵中的p和q是不能交换的，p和q之间不能交距离，只能叫散度，因为二者不满足可交换性。

熵表示的信息量的大小。

比如：中国和巴西踢足球，巴西赢的概率为1，平的概率为0，输的概率为0，这种事情没有信息量，确定性极强，熵为0；

当每种概率都是1/n时，此时熵是最大的，因为结果是不确定的。

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918165738.png)

通过熵，交叉熵，相对熵的公式可以看出：**交叉熵=熵+相对熵**

又因为真实分布的熵为0，此时交叉熵等于相对熵。

因为相对熵又称为KL散度，表示的是两个随机分布之间的差异，且交叉熵等于相对熵，所以可用交叉熵来表示两个随机分布之间的差异。

### 4.对比多类支撑向量机损失

#### 交叉熵损失 vs 多类支撑向量机损失

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918171843.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918171927.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210918171947.png)

很明显，计算出来的交叉熵损失和多类支撑向量机损失的值是不同的。

采用交叉熵损失训练的结果是，某一项分数高时，还要尽可能去压低其余项的分数；

采用多类支撑向量机损失训练的结果是，某一项分数高时，只比其余项高出1即可。

在使用多类支撑向量机损失时，可能会出现loss值几乎没有变但是分类精度却提高了，比如：

【0.35,0.33,0.32】和【0.33,0.35,0.32】，两则计算出的loss值差距很小，但是分类的结果却完全不同。

### 5.计算图与反向传播

#### 计算图

计算图是一种有向图，它用来表达输入、输出以及中 间变量之间的计算关系，图中的每个节点对应着一种数学运算。

当多层神经网络时，写出求导后的表达式是不容易的，引入计算图可以实现复杂函数的求导问题。

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920150534.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920151419.png)

通过计算图计算某处的值，要知道三点：

1. 上图中1处的值，即z=x+y处的z值是多少；
2. 上图中2处求导的表达式，即z^2求导的表达式
3. 上图中3处的导数的值；

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920152101.png)

#### 反向传播

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920152632.png)

总结：

1. 通过正向计算出各处的值；
2. 能反向计算是因为有了链式法则；
3. 反向传播最后一个导数为1，因为f对f求导还是1；
4. 所求处的梯度=上游梯度*局部梯度；
5. 上游梯度是指所求处右侧的梯度；局部梯度为所求处右侧圆圈式子里的梯度；
6. 局部梯度的式子若为+1，即为x+1,若为*-1，即为-x,以此类推；

存在的问题：因为是相乘操作，当梯度都特别小时，乘着乘着就可能出现梯度消失的情况；

#### 计算图的颗粒度

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920153549.png)

计算图的颗粒度：每一个门可以是一个很复杂的函数，也可以是简单的加减乘除；函数越复杂代表颗粒度越大；

简单函数的好处是都可以按照基源去操作，坏处是要严格按照链路一步一步去计算；

复杂的函数颗粒度较大，但计算效率较高；

#### 计算图中常见的门单元

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920154715.png)

- 加法门：左边每个分支的值都等于右边的值；
- 乘法门：两数互换，再相乘；
- 拷贝门：左边等于右边的和；
- max门：把右边的值赋给左边最大的，其余为0；

### 6.再看激活函数

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920211644.png)

#### 梯度消失

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920211751.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920211830.png)

通过观察sigmoid函数和tanh函数的图像可知，当x超出一定值后梯度为0，由于链式法则是相乘的就会出现梯度消失的情况。

为了解决这一问题就出现了Leakly ReLU函数

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920212056.png)

Leakly ReLU函数在x>0的情况下梯度恒为1，在x<0的情况下恒为0.01；效果较好；

#### 梯度爆炸

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920212429.png)

梯度爆炸会导致错过良好的w值；

通过限制步长的方法来解决梯度爆炸；

#### 激活函数选择总结

尽量选择ReLU函数或者Leakly ReLU函数，相对于Sigmoid/tanh， ReLU函数或者Leakly ReLU函数会让梯度流更加顺畅，训练过程收敛 得更快。

由于Sigmoid/tanh函数的特性，一般情况下不会在隐层中使用，但可能会用在输出层，把结果控制在某一个范围中。

### 7.动量法与自适应梯度(梯度算法的改进)

#### 梯度下降算法存在的问题

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920213241.png)

当存在多个w时，传统梯度下降算法会在陡峭处w会来回震荡，而平坦处的w优化缓慢；

仅仅增大步长并不能加快算法收敛的速度，因为w=w-步长*某点处梯度，增加步长会使陡峭的地方震荡幅度增大(因为陡峭处的梯度值的绝对值较大)，平坦的地方进展仍然缓慢(因为平坦处的梯度值的绝对值较小)；

#### 动量法

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920214530.png)

动量是有方向的，梯度也是有方向的；

在陡峭的地方，梯度方向是不断变化的，当把一定的梯度累加在一起可能是在某一个方向上一个很小的数；在平坦的地方，梯度的方向是一定的，虽然它是一个很小的数，但是累加起来就是一个较大的数，从而实现梯度较快更新的目的；

动量法累加梯度信息是图中3处，并不是2处，2处是小批量随机梯度下降算法的累加；

动量系数u控制历史信息是不是要被衰减；

若u=0时就退化成梯度下降算法；若u=1时就会不断的加大历史的影响，即v=v+g,即使梯度下降算法走到平坦的区域(g=0)时，v仍然不等于0，那么权值会仍然更新下去，但若v=0.9时，v=0.9v，一定次数后v的值将会非常小。

u的值一般使用0.9；

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920215559.png)

因为鞍点和局部最小值点的导数都为0，又因为w=w-步长*导数，则w更新不动了，此时通常就认为收敛了，但这是不对的；

加入动量以后，即使局部梯度为0，即g=0,但是v=uv+g不为0，并不会马上停止更新，此时就有可能冲出局部最小点或鞍点 ，找到更优的解。

#### 自适应梯度与RMSProp

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920220019.png)

自适应梯度算法就是在陡峭的地方用小步长，在平坦的地方用大步长。

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920221036.png)

 通过计算梯度平方和找出震荡方向和平坦方向，在4中表达式可以看出较大的r学习率较小，较小的r学习率较大，从而达到减少震荡更快达到谷底的目的。

存在问题：因为r一直在累加，当加到很大时，学习率就会趋近于0，失去调节作用；

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210920220101.png)

RMSProp解决了AdaGrad存在的问题，因为r前面存在ρ,当r加到一定的次数，前面的r就会衰减变得非常小，进而保证r的值永远不会很大。

ρ=0时，仅仅考虑当前梯度的强度；

ρ=1时，r=r，又r初始为0，显示不可能；

ρ一般取0.999；

保留的次数越多，p的值就应该越大，例如：保留一百个数p的值应为0.99(1-1/100=0.99),可以此类推(只是大概而已)；

#### ADAM

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210921163331.png)

ADAM是动量法和自适应梯度下降算法的结合；

图中3处体现了动量法，图中4处体现了自适应梯度下降算法，图中5处解决了程序初期的冷启动问题；

图中5处解决程序刚开始运行时的冷启动问题：当第一次执行程序，u=0.9，v=0,t=1时:图中3处v=0.1g，若此时去更新梯度则实际步长是非常小的，更新较慢，在图中5处进行修正偏差，u=0.9,t=1此时修正后的v=g,从而解决了程序刚开始执行时梯度更新较慢的问题，随着t的增大，u的t次方趋近于0，从而修正偏差逐渐失去作用，不会对开始一段时间后的更新产生影响；r的更新同理。

t代表程序第t次执行；

一般情况下，u=0.9,ρ=0.999，这两个是经验值，可以直接用。

#### 总结

动量法：采用累加梯度的思想，降低陡峭处w的变化幅度，增强平坦处w的变化幅度；

自适用梯度和RMSProp：自适应梯度算法的思想是减小陡峭处的步长，增大平坦处的步长，从而提高优化效率，但是存在问题(因为r一直在累加增大，当r太大时，就会对步长失去调节作用)，为解决这个问题就出现了RMSProp算法，它通过给r设置衰减系数的方法，从而防止r的值不会变的很大。

ADAM：ADAM算法是动量法和自适用梯度算法的结合，又加入了修正偏差，解决了冷启动问题(程序刚开始执行时，梯度更新较慢)。

在一般情况下使用ADAM算法，但是它没有动量法+小批量梯度下降算法手动调优的效果好。ADAM算法在大部分任务下可以速度很快。

在实际使用过程中，有可能先用ADAM算法初始学习一下，学不动了用动量法+SGD手动调；也有可能先用动量法+SGD先学习一下，然后用ADAM做后面的加速。

### 8.权值初始化

#### 全零初始化

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210921202104.png)

全零初始化会使不同的神经元有相同的输出，不能进行正向传播和反向传播；

#### 随机权值初始化

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210921202149.png)

如采用较小的随机正态分布采样，通过双曲正切激活函数的图像可知，激活后几乎所有值趋近于0，在经过激活函数所有值就为0，在后面的隐层不能进行正向传播；

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210921202213.png)

如采用较大的随机正态分布采样，通过双曲正切激活函数的图像可知，激活后几乎所有值趋近于1和-1，此时局部神经元梯度为0，不能进行反向传播；

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210921202249.png)

#### Xavier初试化

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210921202313.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210921202338.png)

(此处公式推导的原理还不明白)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210921202412.png)

N为输入的神经元个数；

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210921202431.png)

#### HE初始化

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210921202452.png)

#### 权值初始化小结

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210921202518.png)

### 9.批归一化

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925140653.png)

批归一化就是把输入先调整成0均值1方差的样子，然后再调整成与输入具有相同的分布。

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925141101.png)

批归一化在非线性激活之前。

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925141157.png)

γ和β是参数，是通过训练得到的；

经过减均值，除方差后，数据会集中在均值两侧，此时梯度较好，几乎不会出现为0的情况；

批归一化的作用是使数据集中在激活函数的中间位置，使梯度不会出现为0的情况；

### 10.欠拟合、过拟合与Dropout

#### 过拟合

过拟合——是指学习时选择的模型所包含的参数过多，以至于出现这一模 型对已知数据预测的很好，但对未知数据预测得很差的现象。这种情况下 模型可能只是记住了训练集数据，而不是学习到了数据特征。 

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925142720.png)

一般当出现在验证集上误差增大时即为训练最优状态。

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925142825.png)

##### 调节模型大小

调节模型的宽度和深度；

##### 正则化

L2正则损失对于大数值的权值向量进行严厉惩罚，鼓励更加分散的权重向量， 使模型倾向于使用所有输入特征做决策，此时的模型泛化性能好；

使分界面变得平滑，不那么复杂；

##### 随机失活

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925143511.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925143602.png)

随机失活鼓励权值分散，因为当某些较大的权值被失活时就可能造成神经网络的瘫痪。

解释三：Dropout可以看作模型的集成，因为随机失活，每个随机失活剩余的模型就不同(则可以看作多个模型的集成)，可能会出现不同的分类结果，可以找到分类结果出现最多的那次(相当于投票)。

#### 欠拟合

欠拟合——模型描述能力太弱，以至于不能很好地学习到数据中的规律。 产生欠拟合的原因通常是模型过于简单。

### 11.模型正则与超参数优化

#### 神经网络中的超参数

- 网络结构——隐层神经元个数，网络层数，非线性单元选择等；

- 优化相关——学习率、dropout比率、正则项强度等；

#### 学习率设置

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925145211.png)

#### 参数优化方法

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925145300.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925145343.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925145404.png)

#### 超参数的标尺空间

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20210925145439.png)