---
title: 线性回归模型
tags:
  - 计算机视觉
categories: 计算机视觉
abbrlink: 21594
date: 2022-04-19 19:52:26
---
```python
import numpy as np
import torch
import torch.nn as nn

x_values = [i for i in range(11)]
x_train = np.array(x_values, dtype=np.float32)
x_train = x_train.reshape(-1, 1)
print(x_train.shape)

y_values = [2 * i + 1 for i in x_values]
y_train = np.array(y_values, dtype=np.float32)
y_train = y_train.reshape(-1, 1)
print(y_train.shape)


# 线性回归模型(其实线性回归就是一个不加激活函数的全连接层)

class LinearRegressionModel(nn.Module): # nn.Module父类
    # 前面介绍了如何自定义一个模型——通过继承nn.Module类来实现，
    # 在__init__构造函数中申明各个层的定义，在forward中实现层之间的连接关系，实际上就是前向传播的过程。
    def __init__(self, input_dim, output_dim):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)

    # 实例化一个对象中传入对应的参数就可以自动调用forward函数,是在进行前向传播时调用的，而不是在__init__后就直接调用的
    def forward(self, x):
        out = self.linear(x)
        return out


# input_dim和output_dim是输入数据的维度和输出数据的维度
input_dim = 1
output_dim = 1
model = LinearRegressionModel(input_dim, output_dim)

epochs = 1000
learning_rate = 0.01
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # optimizer 优化器
# 损失函数一般看任务来说吧  分类任务一般都是交叉熵，回归任务一般条件下是MSE
criterion = nn.MSELoss()    # criterion 标准

# 训练模型
for epoch in range(epochs):
    epoch += 1
    # 注意转换成tensor
    inputs = torch.from_numpy(x_train)
    labels = torch.from_numpy(y_train)

    # 梯度要清零每一次迭代
    optimizer.zero_grad()

    # 前向传播
    outputs = model(inputs)

    # 计算损失
    loss = criterion(outputs, labels)

    # 反向传播
    loss.backward()

    # 更新权重参数
    optimizer.step()

    if epoch % 50 == 0:
        print("epoch {},loss {}".format(epoch, loss.item()))

# 利用模型去预测结果
predicted = model(torch.from_numpy(x_train).requires_grad_()).data.numpy()
print(predicted)

```

