---
title: 视觉识别
tags:
  - 计算机视觉
categories: 计算机视觉
abbrlink: 49853
date: 2021-10-11 18:29:37
---

### 视觉识别任务

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211011183222.png)

- 分类：不考虑空间的位置；
- 语义分割：给我一个图像，告诉我图像上每一像素是什么语义；
- 目标检测：图像上某个区域是什么类别(是目标还是背景，是目标的话是什么标签)
- 实例分割：在同一个图像中，同一类别不同物品显示不同的颜色，即若一个图像中有两个猫，它会用不同的颜色表示两只猫；

实例分割与语义分割不同的是：语义分割只区分同一种类，不区分同一种类中的不同。

### 语义分割

#### 定义

给每个像素分配类别标签不区分实例，只考虑像素类别。

#### 语义分割思路

##### 滑动窗口

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211011184146.png)

具体过程：在一张图片上，通过滑动窗口的方式对每个点取周围的区域进行分类判断；

存在的问题：效率太低；

##### 全卷积

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211011184526.png)

C的大小为类别个数；

具体过程：给输入的特征图增加padding，使其保持原有大小；输出为C * H * w的特征图，可以看成H * W个C维向量，每个C维向量只有一个类别；并且把标答也做成C * H * W，利用交叉熵进行比较，并进行反向传播；

但同时也存在问题；

##### 改进的全卷积

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211011185622.png)

###### 下采样

下采样可以通过padding或增大步长的方式实现；

###### 上采样

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211011190009.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211011190115.png)

把1,2,3,4放到5,6,7,8的位置；

###### 转置卷积

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211011191630.png)

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211011190836.png)

x,y,z是三个参数；左右两图的两组x,y,z参数是不同的；

##### 过程

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211013164717.png)

- 传统的，在上采样过程中为了防止信息的丢失，直接把下采样过程中对应的特征图和上采样过程中对应的特征图拼接起来；
- 最优的，先把下采样过程中得到的特征图经过卷积进行卷积，然后再拼接起来，保证正向的一些信息流能够传递过来；

### 目标检测

#### 定义

不光告诉我们图像中有什么目标，还要告诉我们它在什么位置；

#### 思路

在倒数第二层的全连接层上分别接上分类器和确定位置的神经网络；

当做目标定位这种网络时，找到一个预先训练好的模型，然后直接把它分类，然后在这个训练的基础上，训练倒数第二层与定位层直接连接的参数，让它的总损失降到最低；

在实际的训练过程中可以这样，先训练好分类，然后所有参数不变去训练定位层的参数，然后再把所有参数都打开进行微调，使总损失下降；

#### 损失值

因为有分类和定位，所有又称多任务损失；

分类的损失很容易计算，那么定位的损失如何计算呢？

x,y,h,w相差的平方和。

在计算两个损失和时，可以设置权重，来倾向于哪个更拟合正确；

#### 多目标检测

当多个目标时，就难以知道标答的计算，采用单目标分类定位的方法是不可行的；只有当目标个数确定时，才能用单目标这种方法进行；

即因为不知道目标的个数，就难以确定输出的维度；

##### 方法1：滑动窗口

存在的问题：CNN需要对图像中所有可能的区域（不同 位置、尺寸、长宽比）进行分类，计算量巨大！

##### 方法2：R-CNN

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211013182827.png)

- 找出所有潜在可能包含目标的区域；
- 运行速度需要相对较快；

利用区域建议方法产生感兴趣的区域，并对该区域进行缩放到224 * 224，因为输入要求是224 * 224，然后再把缩放后的图像进行特征提取；随后进行Bbox reg进行坐标回归和使用支持向量机对区域进行分类；

通过Bbox reg回归选取和实际的误差，调节选取窗口的参数，解决了选取区域不准确的问题；

存在的问题：计算效率低下；

解决方法：在特征图上进行区域扣取；

##### 方法3：Fast R-CNN

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211013183112.png)

###### 思想

先对整图进行卷积操作，然后再进行区域裁剪和缩放特征；

要保证裁剪和缩放都可导，整个网络才能训练。

###### 区域裁剪: RoI Pool

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211013183741.png)

###### 区域裁剪: RoI Align

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211013184053.png)

与Rol Pool相比，保持映射过来的区域，平均选取四个点，进行计算该点的值，再采用最大池化的方式进行提取；

如何计算选点处的值？

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211013184225.png)

##### 方法4：Faster R-CNN

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211013185320.png)



###### 思路

在中间特征层后加入区域建议网络RPN( Region Proposal Network) 产生候选区域，其他部分保持与Fast R-CNN一致，即 扣取每个候选区域的特征，然后对其进行分类。

###### 区域建议（Region Proposal Network）

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211013190034.png)

对每一个像素都要在它外面圈一个框，这个框的分类是什么，这代表这个位置的分类就是什么(是不是目标)；

问题：框选取多大，框长什么样？

在编程的时候选取的都是固定的区域，但是可以通过回归出来和真正需要选择区域的偏差量，调整区域框；

4k的4：每个像素回归4个数字；

对于选取的K * 20 * 15 的边框，选取最合适的前300个边框进行目标检测；

###### 四种损失

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211013190326.png)

RPN分类损失只是判断该区域是不是目标；

###### 应用

![image-20211013212211838](C:\Users\RomanticQq\AppData\Roaming\Typora\typora-user-images\image-20211013212211838.png)

整个训练过程：先对整个图像进行卷积，得到特征图，然后分别对每个像素点进行区域划分回归，选择就有可能是目标区域的300个区域进行区域裁剪，然后再进行分类和区域定位。

![image-20211013212736996](C:\Users\RomanticQq\AppData\Roaming\Typora\typora-user-images\image-20211013212736996.png)

YOLO是速度比较快的，虽然可能精度上没有Faster R-CNN高，但是性价比是比较高的；

###### 影响因素

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211013213651.png)

#### 实例分割

![](https://myforpicgo.oss-cn-beijing.aliyuncs.com/image/20211013213815.png)